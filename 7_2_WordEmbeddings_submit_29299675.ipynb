{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7_2_WordEmbeddings_submit_29299675.ipynb","provenance":[{"file_id":"https://github.com/ecs-vlc/COMP6248/blob/master/docs/labs/lab7/7_2_WordEmbeddings.ipynb","timestamp":1555800068107}],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"984959cd4675237d2964d0036770360d","grade":false,"grade_id":"cell-6f17f4f5348cd26a","locked":true,"schema_version":1,"solution":false},"id":"OAILnU4L6GPh"},"source":["# Part 2: Word Embeddings"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"09ae37aba80f8a3d948cadbed7991dee","grade":false,"grade_id":"cell-426d00926ef2a948","locked":true,"schema_version":1,"solution":false},"id":"GkTFMmpb6GPm","executionInfo":{"status":"ok","timestamp":1555876352966,"user_tz":-60,"elapsed":8878,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"ea7a26ab-82db-471f-88f7-ed5964f4fe45","colab":{"base_uri":"https://localhost:8080/","height":320}},"source":["# Execute this code block to install dependencies when running on colab\n","try:\n","    import torch\n","except:\n","    from os.path import exists\n","    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","\n","    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n","\n","try: \n","    import torchbearer\n","except:\n","    !pip install torchbearer\n","    \n","try:\n","    import torchtext\n","except:\n","    !pip install torchtext\n","    \n","try:\n","    import spacy\n","except:\n","    !pip install spacy\n","\n","try:\n","    spacy.load('en')\n","except:\n","    !python -m spacy download en"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting torchbearer\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/62/79c45d98e22e87b44c9b354d1b050526de80ac8a4da777126b7c86c2bb3e/torchbearer-0.3.0.tar.gz (84kB)\n","\u001b[K    100% |████████████████████████████████| 92kB 4.2MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4 in /usr/local/lib/python3.6/dist-packages (from torchbearer) (1.0.1.post2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from torchbearer) (0.2.2.post3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchbearer) (4.28.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision->torchbearer) (1.16.2)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->torchbearer) (4.3.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision->torchbearer) (1.11.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision->torchbearer) (0.46)\n","Building wheels for collected packages: torchbearer\n","  Building wheel for torchbearer (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/6c/cb/69/466aef9cee879fb8f645bd602e34d45e754fb3dee2cb1a877a\n","Successfully built torchbearer\n","Installing collected packages: torchbearer\n","Successfully installed torchbearer-0.3.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"d61a9db4eae318662e2d525b44722941","grade":false,"grade_id":"cell-cc5d1e090cea2dd0","locked":true,"schema_version":1,"solution":false},"id":"yQ0ub8UH6GPs"},"source":["Word embeddings transform a one-hot encoded vector (a vector that is 0 in elements except one, which is 1) into a much smaller dimension vector of real numbers. The one-hot encoded vector is a *sparse vector*, whilst the real valued vector is a *dense vector*. \n","\n","The key concept in these word embeddings is that words that appear in similar _contexts_ appear nearby in the vector space, i.e. the Euclidean distance between these two word vectors is small. By context here, we mean the surrounding words. For example in the sentences \"I purchased some items at the shop\" and \"I purchased some items at the store\" the words 'shop' and 'store' appear in the same context and thus should be close together in vector space.\n","\n","We'll talk about some of the well-known algorithms for learning embeddings in the lectures, but you might have already heard of a popular model called *word2vec*, which was first published in a rejected ICLR submission (it has some pretty damning reviews, but also has thousands of citations!). In this lab we'll use pre-trained *GloVe* vectors. *GloVe* is a different algorithm for computing word vectors, although the outcome is similar to *word2vec*. These pre-trained embeddings have been trained on a gigantic corpus. We can use these pre-trained vectors within any of our models, with the idea that as they have already learned the context of each word they will give us a better starting point for our word vectors. This usually leads to faster training time and/or improved accuracy.\n","\n","In PyTorch, we use word vectors with the `nn.Embedding` layer, which takes a _**[sentence length, batch size]**_ tensor and transforms it into a _**[sentence length, batch size, embedding dimensions]**_ tensor. `nn.Embedding` layers can be trained from scratch, or they can be initialised (and optionally fixed) with pre-trained embedding data. The key thing to remember about an `nn.Embedding` is that it does not need to explicitly use a one-hot vector representation at any point; it just maps an index to a vector. This is important because it implies massive computational savings; more concretly an Emdedding is essentially a linear map in which **the weight matrix of the linear layer is multiplied by a one-hot sparse-vector to produce a lower-dimensional (dense) output**. This is exactly equivalent to just **selecting the column of the weight matrix corresponding to the index represented by the sparse vector**.\n","\n","In this part of the lab we won't be training any models; instead we'll be looking at the word embeddings and investigating a few interesting things we can do with them.\n","\n","## Loading the GloVe vectors\n","\n","First, we'll load the pre-trained GloVe vectors. The `name` field specifies what the vectors have been trained on, here the `6B` means a corpus of 6 billion words. The `dim` argument specifies the dimensionality of the word vectors. GloVe vectors are available in 50, 100, 200 and 300 dimensions. There is also a `42B` and `840B` glove vectors, however they are only available at 300 dimensions. The first time you run this it will take time as the vectors need to be downloaded:"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"ad157a2a3b61280e0b6f17bd3ad12faa","grade":false,"grade_id":"cell-aceafa53d8c3ee9e","locked":true,"schema_version":1,"solution":false},"id":"JOcE3Re06GPt","executionInfo":{"status":"ok","timestamp":1555876401839,"user_tz":-60,"elapsed":57713,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"f87b0897-c8f6-4320-eb89-6509df74d84b","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["import torchtext.vocab\n","\n","glove = torchtext.vocab.GloVe(name='6B', dim=100)\n","\n","print(f'There are {len(glove.itos)} words in the vocabulary')"],"execution_count":null,"outputs":[{"output_type":"stream","text":[".vector_cache/glove.6B.zip: 862MB [00:13, 63.0MB/s]                           \n","100%|█████████▉| 398945/400000 [00:14<00:00, 26992.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["There are 400000 words in the vocabulary\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"6d6d3a9d7c029ce69fbc5b4b65d8dcf7","grade":false,"grade_id":"cell-504c748c992c4304","locked":true,"schema_version":1,"solution":false},"id":"TYY_L_dQ6GPx"},"source":["As shown above, there are 400,000 unique words in the GloVe vocabulary. These are the most common words found in the corpus the vectors were trained on. **In these set of GloVe vectors, every single word is lower-case only.**\n","\n","`glove.vectors` is the actual tensor containing the values of the embeddings."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"5c17fc906f39bcc8ad50be640173a9fa","grade":false,"grade_id":"cell-d581dee4f722cca5","locked":true,"schema_version":1,"solution":false},"id":"dP4c_s8q6GPy","executionInfo":{"status":"ok","timestamp":1555876401840,"user_tz":-60,"elapsed":57682,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"583416be-0b25-49c0-89d3-c5be3cfb9caa","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["glove.vectors.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([400000, 100])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"a45800d85ac4b4b0b28b7c5786e7f611","grade":false,"grade_id":"cell-8b7f0ea5ae20558b","locked":true,"schema_version":1,"solution":false},"id":"yYVZUswt6GP3"},"source":["We can see what word is associated with each row by checking the `itos` (int to string) list. \n","\n","Below implies that row 0 is the vector associated with the word 'the', row 1 for ',' (comma), row 2 for '.' (period), etc."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"056f9034fdc1ea90e5eb7f2aec199e04","grade":false,"grade_id":"cell-c729cc0c78c40e6c","locked":true,"schema_version":1,"solution":false},"id":"m1Dje9nQ6GP4","executionInfo":{"status":"ok","timestamp":1555876402793,"user_tz":-60,"elapsed":58603,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"20b36377-6feb-4687-c711-eef442807d8b","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["glove.itos[:10]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"d54851b373cca20e645497dfe6dfc8f0","grade":false,"grade_id":"cell-f0e2bc91e46d85ab","locked":true,"schema_version":1,"solution":false},"id":"4xNxK3e26GP8"},"source":["We can also use the `stoi` (string to int) dictionary, in which we input a word and receive the associated integer/index. If you try get the index of a word that is not in the vocabulary, you receive an error."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"4328787e224e941b4dcc53c2315626ee","grade":false,"grade_id":"cell-a95267b694432ed1","locked":true,"schema_version":1,"solution":false},"id":"L2wiyLqx6GP-","executionInfo":{"status":"ok","timestamp":1555876402800,"user_tz":-60,"elapsed":58588,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"6315a9d7-11c4-4d50-a788-1c7f2c79b83a","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["glove.stoi['the']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"b506644789934cd8b5a7afcc4e3ccc39","grade":false,"grade_id":"cell-ba0310634f767896","locked":true,"schema_version":1,"solution":false},"id":"SgUPlVvD6GQB"},"source":["We can get the vector of a word by first getting the integer associated with it and then indexing into the word embedding tensor with that index."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"bbc1b6a3ec1c87301f8b913034b5798c","grade":false,"grade_id":"cell-ec58401ce38b8fe9","locked":true,"schema_version":1,"solution":false},"id":"3LFYdlin6GQD","executionInfo":{"status":"ok","timestamp":1555876402802,"user_tz":-60,"elapsed":58573,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"e7d12e8a-57ab-4d19-f824-941e4eb53e1e","colab":{"base_uri":"https://localhost:8080/","height":247}},"source":["glove.vectors[glove.stoi['the']]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n","        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n","         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n","         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n","         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n","        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n","        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n","         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n","         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n","        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n","         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n","         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n","        -0.5203, -0.1459,  0.8278,  0.2706])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"d9591f8c7576a4211a24259d5a08f495","grade":false,"grade_id":"cell-3f6513c100744743","locked":true,"schema_version":1,"solution":false},"id":"XgmDPIhY6GQH"},"source":["We'll be doing this a lot. __Use the following block to create a function that takes in word embeddings and a word and returns the associated vector.__ You should throw an error if the word doesn't exist in the vocabulary:"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"cc248cb64c4760e65292e3e7a8c4b3cd","grade":true,"grade_id":"cell-665d7b4d1dd8f339","locked":false,"points":4,"schema_version":1,"solution":true},"id":"5ak4xDbh6GQI"},"source":["def get_vector(embeddings, word):\n","    # YOUR CODE HERE\n","    vec=embeddings.vectors[embeddings.stoi[word]]\n","    return vec"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"ca9447056d8b427e99b7bd0372902d9c","grade":false,"grade_id":"cell-9ff707bac4fd2556","locked":true,"schema_version":1,"solution":false},"id":"vg6JF2KQ6GQP"},"source":["As before, we use a word to get the associated vector."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"fdf27ce0601aef03ff1efdd73bd779ca","grade":false,"grade_id":"cell-6311b14c496949f5","locked":true,"schema_version":1,"solution":false},"id":"9vuV25kF6GQQ","executionInfo":{"status":"ok","timestamp":1555876402807,"user_tz":-60,"elapsed":58549,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"91928d74-3b1c-4600-eb8b-e61edd46a8d4","colab":{"base_uri":"https://localhost:8080/","height":247}},"source":["get_vector(glove, 'the')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n","        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n","         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n","         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n","         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n","        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n","        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n","         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n","         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n","        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n","         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n","         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n","        -0.5203, -0.1459,  0.8278,  0.2706])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"sXFYzkmRoUBe","executionInfo":{"status":"ok","timestamp":1555876402808,"user_tz":-60,"elapsed":58531,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"0456db3d-e070-48cd-d9ff-299b4236a65e","colab":{"base_uri":"https://localhost:8080/","height":247}},"source":["get_vector(glove, 'psychology')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.3368, -0.0911, -0.0692,  0.4674,  0.0844,  0.1594,  0.8618, -0.6372,\n","        -0.5189,  0.3833, -0.4894, -0.5075,  0.2292,  0.8544,  0.3305,  0.3963,\n","        -0.0859,  0.3472, -0.3675,  0.4605, -0.1167,  0.3596, -0.0196, -0.1499,\n","        -0.1028, -0.2677,  0.5130, -1.2081, -0.0449,  0.4131, -1.0296,  0.9388,\n","        -0.9692,  0.2968, -1.1742, -0.2559, -0.4738,  0.6550, -0.3848, -0.0995,\n","        -0.5413, -0.3532, -0.8416, -0.6348, -0.9781, -0.1107,  0.4187,  0.6557,\n","        -0.3317,  0.0244,  0.5875, -0.1932,  0.0639, -0.0160,  0.5082, -0.6654,\n","         0.7339, -0.5878,  0.8961,  0.2330,  0.5344,  0.6424,  0.1732, -0.7750,\n","         0.8090,  0.5017,  0.2485, -0.3271,  0.8510,  1.1980, -0.3778,  0.6577,\n","        -0.2074,  0.3666, -1.0243, -0.0086,  0.1417,  0.0621, -0.1975, -0.8854,\n","        -0.6022,  0.7316, -0.6599, -0.4851, -0.8997,  0.3334, -0.3006, -0.7379,\n","        -0.3255, -0.9026,  0.3839, -0.2596,  0.3236,  0.5598,  0.9847,  0.5924,\n","         0.0738, -1.1037,  0.6259,  0.8680])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"_PmHA4aloZmI","executionInfo":{"status":"ok","timestamp":1555876402809,"user_tz":-60,"elapsed":58515,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"01b979e9-26ba-49e5-d86b-4d3bee7ed6b8","colab":{"base_uri":"https://localhost:8080/","height":247}},"source":["get_vector(glove, 'jackey')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 0.0712, -0.3945, -0.7177,  0.1023, -0.0200, -0.4950,  0.1156, -0.7261,\n","         0.4314, -0.6186,  0.0341, -0.1388,  0.1995, -0.0323,  0.2351, -0.1396,\n","        -0.3544,  0.3488,  0.1324,  0.0870, -0.3068,  0.0991, -0.4893, -0.0868,\n","        -0.2679,  0.4805, -0.9967,  0.2810, -0.0668,  0.2246, -0.1346, -0.4247,\n","         0.5184,  0.2846,  0.7231,  0.2681, -0.4695, -0.5003,  0.0921, -0.1656,\n","        -0.0533,  0.1650, -0.1488,  0.5162, -0.2793,  0.2286, -0.1661,  0.8253,\n","         0.5716,  0.9620, -0.4374,  0.5809, -0.5433, -0.4543,  0.1657,  1.3532,\n","        -0.0017, -0.0745, -0.7277, -0.5155, -0.1521, -0.2838,  0.0910,  0.5440,\n","        -0.2246,  0.1221, -0.0574, -0.2189, -0.0462,  0.1743,  0.0364,  0.1582,\n","         0.3690,  0.3867, -0.4058,  0.0184, -0.0402,  0.0705,  0.1114, -0.1351,\n","        -0.2158,  0.0218,  0.0033, -0.4998,  0.4412,  0.0779,  0.3565,  0.1167,\n","        -0.4824,  0.5996, -0.0147, -0.4187,  0.2110, -0.0578,  0.4604, -0.4112,\n","        -0.5791,  0.4431, -0.7654, -0.2720])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"db7798ec9825a17c0ccede676217ccff","grade":false,"grade_id":"cell-5ce5018fe3a64aa5","locked":true,"schema_version":1,"solution":false},"id":"D-2I1KJN6GQT"},"source":["## Similar Contexts\n","\n","Now to start looking at the context of different words. \n","\n","If we want to find the words similar to a certain input word, we first find the vector of this input word, then we scan through our vocabulary finding any vectors similar to this input word vector.\n","\n","The function below returns the closest 10 words to an input word vector:"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"e0c40de99156af4a0ccbfde1e6c8c73c","grade":false,"grade_id":"cell-7caf2bf37d7f6acd","locked":true,"schema_version":1,"solution":false},"id":"XSGwnZ1q6GQU"},"source":["import torch\n","\n","def closest_words(embeddings, vector, n=10):\n","    distances = [(w, torch.dist(vector, get_vector(embeddings, w)).item()) for w in embeddings.itos]\n","    return sorted(distances, key = lambda w: w[1])[:n]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"67ad88185491caa67edc4c043b2dfffa","grade":false,"grade_id":"cell-37ae589db1788914","locked":true,"schema_version":1,"solution":false},"id":"fshtEpwC6GQa"},"source":["Let's try it out with 'korea'. The closest word is the word 'korea' itself (not very interesting), however all of the words are related in some way. Pyongyang is the capital of North Korea, DPRK is the official name of North Korea, etc.\n","\n","Interestingly, we also get 'Japan' and 'China',  implies that Korea, Japan and China are frequently talked about together in similar contexts. This makes sense as they are geographically situated near each other. "]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"3c53201c7b9717f670c96e6db2ce1fc0","grade":false,"grade_id":"cell-f0315fd8a79e03c2","locked":true,"schema_version":1,"solution":false},"id":"du7hwEDS6GQb","executionInfo":{"status":"ok","timestamp":1555876406328,"user_tz":-60,"elapsed":61988,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"0ec9debf-58eb-4dd0-9e66-2ec422348a53","colab":{"base_uri":"https://localhost:8080/","height":194}},"source":["closest_words(glove, get_vector(glove, 'korea'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('korea', 0.0),\n"," ('pyongyang', 3.9039554595947266),\n"," ('korean', 4.068886756896973),\n"," ('dprk', 4.2631049156188965),\n"," ('seoul', 4.340494155883789),\n"," ('japan', 4.551243782043457),\n"," ('koreans', 4.615609169006348),\n"," ('south', 4.65822696685791),\n"," ('china', 4.839518070220947),\n"," ('north', 4.986356735229492)]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"cc290c1df8992513c81e9f6ab7d30812","grade":false,"grade_id":"cell-aa3ce8cccc903a24","locked":true,"schema_version":1,"solution":false},"id":"HBcGfWvr6GQg"},"source":["Looking at another country, India, we also get nearby countries: Thailand, Malaysia and Sri Lanka (as two separate words). Australia is relatively close to India (geographically), but Thailand and Malaysia are closer. So why is Australia closer to India in vector space? A plausible explaination is that India and Australia appear together in the context of [cricket](https://en.wikipedia.org/wiki/Cricket) matches."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"5b12751bffec3905109d45bdf5503e5f","grade":false,"grade_id":"cell-e42121c7951db2d3","locked":true,"schema_version":1,"solution":false},"id":"BIDDiZ1Z6GQh","executionInfo":{"status":"ok","timestamp":1555876410231,"user_tz":-60,"elapsed":65865,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"b1b9c464-b7e8-4cda-9804-4b7dd878b637","colab":{"base_uri":"https://localhost:8080/","height":194}},"source":["closest_words(glove, get_vector(glove, 'india'))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('india', 0.0),\n"," ('pakistan', 3.6954822540283203),\n"," ('indian', 4.114313125610352),\n"," ('delhi', 4.155975818634033),\n"," ('bangladesh', 4.261017799377441),\n"," ('lanka', 4.435845851898193),\n"," ('sri', 4.515716552734375),\n"," ('australia', 4.806082725524902),\n"," ('thailand', 4.994781017303467),\n"," ('malaysia', 5.009334087371826)]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"69ad20637fe1ba949c18119468a615e2","grade":false,"grade_id":"cell-7aa75c705fe39e2b","locked":true,"schema_version":1,"solution":false},"id":"LO7wY0wR6GQn"},"source":["We'll also create another function that will nicely print out the tuples returned by our closest_words function."]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"d38ea85b7b473d4405eb968377727e9e","grade":false,"grade_id":"cell-e0e61a784e7b5c03","locked":true,"schema_version":1,"solution":false},"id":"O6Y6jNdo6GQo"},"source":["def print_tuples(tuples):\n","    for w, d in tuples:\n","        print(f'({d:02.04f}) {w}') "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"c314c093d0931d12e5e56b0473de07b4","grade":false,"grade_id":"cell-158d48e3c33227e7","locked":true,"schema_version":1,"solution":false},"id":"HzP_ykZJ6GQs"},"source":["Using the `print_tuples` function use the code block below to print out the 10 neighbours of 'jaguar':"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"d396d6fedd051278308325893bff1a7c","grade":true,"grade_id":"cell-c883f0e0c2c194b4","locked":false,"points":2,"schema_version":1,"solution":true},"id":"eOwpsLi16GQu","executionInfo":{"status":"ok","timestamp":1555876414474,"user_tz":-60,"elapsed":70066,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"2b9e7bbc-0d1d-497a-c394-72efd459014d","colab":{"base_uri":"https://localhost:8080/","height":194}},"source":["# YOUR CODE HERE\n","words_tup=closest_words(glove, get_vector(glove, 'jaguar'))\n","print_tuples(words_tup)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(0.0000) jaguar\n","(4.0384) rover\n","(4.2649) mustang\n","(4.3939) e-type\n","(4.4494) xk8\n","(4.4579) xjs\n","(4.4906) xj6\n","(4.5109) xkr\n","(4.5336) sepecat\n","(4.5409) xk120\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"5efa0cdc77c06d8a495a6832ac7a2bd3","grade":false,"grade_id":"cell-3964ab858fdb82d8","locked":true,"schema_version":1,"solution":false},"id":"Qw3wt6im6GQx"},"source":["__Use the following block to explain the results.__ (hint: use Google if you don't know what any of the terms are!)"]},{"cell_type":"markdown","metadata":{"deletable":false,"nbgrader":{"checksum":"ece0ec94dff4b1e0bef5bacbec0db9f1","grade":true,"grade_id":"cell-ebf08e96b0987860","locked":false,"points":5,"schema_version":1,"solution":true},"id":"N7zKs2WP6GQy"},"source":["**ANSWER: \"Jaguar\", \"Rover\", \"Mustang\" are famous three bands of off-road vehicle. \"E-type\" is an engine type, \"xk8\" , \"xjs\", \"xj6\", \"xkr\", \"sepecat\", and \"xk120\" are corresponding  model number of this kind of engine. This shows the relationship among these words.**"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"37e34f44b1798d1f74fb49210ef1da43","grade":false,"grade_id":"cell-9f417bc0fb8d5287","locked":true,"schema_version":1,"solution":false},"id":"Cx3o2HIn6GQ0"},"source":["## Analogies\n","\n","Another property of word embeddings is that we can apply standard arithmetic operations. This can give interesting results.\n","\n","We'll show an example of this first, and then explain it:"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"895399907963f0f7a83448dec5165c5d","grade":false,"grade_id":"cell-a0f613c8739e5a3c","locked":true,"schema_version":1,"solution":false},"id":"KQ91UoaH6GQ2"},"source":["def analogy(embeddings, word1, word2, word3, n=5):\n","    \n","    candidate_words = closest_words(embeddings, get_vector(embeddings, word2) - get_vector(embeddings, word1) + get_vector(embeddings, word3), n+3)\n","    \n","    candidate_words = [x for x in candidate_words if x[0] not in [word1, word2, word3]][:n]\n","    \n","    print(f'{word1} is to {word2} as {word3} is to...')\n","    \n","    return candidate_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"9994548d295d48b18a1767ebb094a3e6","grade":false,"grade_id":"cell-c197612b5ec94762","locked":true,"schema_version":1,"solution":false},"id":"QnDL5r5p6GQ6","executionInfo":{"status":"ok","timestamp":1555876417800,"user_tz":-60,"elapsed":73356,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"2685da62-8e95-47b5-c03b-147ad797a674","colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["print_tuples(analogy(glove, 'man', 'king', 'woman'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r100%|█████████▉| 398945/400000 [00:30<00:00, 26992.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["man is to king as woman is to...\n","(4.0811) queen\n","(4.6429) monarch\n","(4.9055) throne\n","(4.9216) elizabeth\n","(4.9811) prince\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"5596f944b3ea0977744e61ac0dc9691e","grade":false,"grade_id":"cell-a388955bf35cd584","locked":true,"schema_version":1,"solution":false},"id":"tVIPuklm6GRE"},"source":["This is the canonical example which shows off this property of word embeddings. So why does it work? Why does the vector of 'woman' added to the vector of 'king' minus the vector of 'man' give us 'queen'?\n","\n","If we think about it, the vector calculated from 'king' minus 'man' gives us a \"royalty vector\". This is the vector associated with traveling from a man to his royal counterpart, a king. If we add this \"royality vector\" to 'woman', this should travel to her royal equivalent, which is a queen!\n","\n","We can do this with other analogies too. For example, this gets an \"acting career vector\":"]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"e3ba3a850f1feed61e5f60a7f998ddd3","grade":false,"grade_id":"cell-733267c1033df3c0","locked":true,"schema_version":1,"solution":false},"id":"3kNayTtu6GRF","executionInfo":{"status":"ok","timestamp":1555876506209,"user_tz":-60,"elapsed":4056,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"668ddadc-783e-4987-fa4d-b2c5ed9ed207","colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["print_tuples(analogy(glove, 'man', 'actor', 'woman'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["man is to actor as woman is to...\n","(2.8133) actress\n","(5.0039) comedian\n","(5.1399) actresses\n","(5.2773) starred\n","(5.3085) screenwriter\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"a865a1470a970a0636a4d3fae76cab84","grade":false,"grade_id":"cell-3a873ba9cf3355e4","locked":true,"schema_version":1,"solution":false},"id":"dfSoP8On6GRK"},"source":["__Use the following block to compute a 'capital city vector' that predicts the capital of England based on the capital and name of another country__:"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"328c71eb5601837b5202b85934e9bb1c","grade":true,"grade_id":"cell-9f399ee568e4ae5b","locked":false,"points":2,"schema_version":1,"solution":true},"id":"bQnkJTHP6GRK","executionInfo":{"status":"ok","timestamp":1555877065287,"user_tz":-60,"elapsed":4017,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"4e660458-8ea6-4353-fd9a-6a68bbf5052c","colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["# YOUR CODE HERE\n","print_tuples(analogy(glove, 'germany', 'berlin', 'england'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["germany is to berlin as england is to...\n","(4.3972) edinburgh\n","(4.4668) birmingham\n","(4.5694) glasgow\n","(4.5941) leeds\n","(4.7603) melbourne\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"nbgrader":{"checksum":"33e404a915d22e882399fb954ab2ebef","grade":false,"grade_id":"cell-3dbff5286a5d4d7e","locked":true,"schema_version":1,"solution":false},"id":"DoIMmX4t6GRO"},"source":["__Use the following block to compute an 'musical genre vector' that predicts the genre of music played by Eminem based on another musician/band and their genre__:"]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"checksum":"5f8f424d440dd6dcd184b16b8c8be299","grade":true,"grade_id":"cell-d04e647ab1bc99c8","locked":false,"points":2,"schema_version":1,"solution":true},"id":"_-gj3joH6GRP","executionInfo":{"status":"ok","timestamp":1555877299638,"user_tz":-60,"elapsed":4279,"user":{"displayName":"Jing Meng","photoUrl":"","userId":"00936490783355948018"}},"outputId":"314968f3-86f2-4995-cba6-a41cab67a018","colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["# YOUR CODE HERE\n","print_tuples(analogy(glove, 'beethoven', 'classical', 'eminem'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["beethoven is to classical as eminem is to...\n","(6.2551) rap\n","(6.3852) rappers\n","(6.4138) pop\n","(6.4819) hip-hop\n","(6.5143) contemporary\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MvkgHTglghpj"},"source":[""],"execution_count":null,"outputs":[]}]}